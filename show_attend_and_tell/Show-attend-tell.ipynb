{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sklearn\n",
    "from keras.preprocessing import image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from keras.applications import InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense,GRU,Embedding,concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2513260012_03d33305cf.jpg</th>\n",
       "      <th>&lt;start&gt; A black dog is running after a white dog in the snow . &lt;end&gt;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2513260012_03d33305cf.jpg</td>\n",
       "      <td>&lt;start&gt; Black dog chasing brown dog through sn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2513260012_03d33305cf.jpg</td>\n",
       "      <td>&lt;start&gt; Two dogs chase each other across the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2513260012_03d33305cf.jpg</td>\n",
       "      <td>&lt;start&gt; Two dogs play together in the snow . &lt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2513260012_03d33305cf.jpg</td>\n",
       "      <td>&lt;start&gt; Two dogs running through a low lying b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2903617548_d3e38d7f88.jpg</td>\n",
       "      <td>&lt;start&gt; A little baby plays croquet . &lt;end&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2513260012_03d33305cf.jpg  \\\n",
       "0  2513260012_03d33305cf.jpg   \n",
       "1  2513260012_03d33305cf.jpg   \n",
       "2  2513260012_03d33305cf.jpg   \n",
       "3  2513260012_03d33305cf.jpg   \n",
       "4  2903617548_d3e38d7f88.jpg   \n",
       "\n",
       "  <start> A black dog is running after a white dog in the snow . <end>  \n",
       "0  <start> Black dog chasing brown dog through sn...                    \n",
       "1  <start> Two dogs chase each other across the s...                    \n",
       "2  <start> Two dogs play together in the snow . <...                    \n",
       "3  <start> Two dogs running through a low lying b...                    \n",
       "4        <start> A little baby plays croquet . <end>                    "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData = pd.read_csv(\"/home/nishimehta15/NIC/flickr/trainImages.txt\", delimiter='\\t')\n",
    "images_path = \"/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/\"\n",
    "trainData.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "image_names = []\n",
    "for row in trainData.iterrows():\n",
    "    captions.append(row[1][1])\n",
    "    image_names.append(images_path + str(row[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData = pd.read_csv(\"/home/nishimehta15/NIC/flickr/testImages.txt\", delimiter='\\t')\n",
    "testData.head(5)\n",
    "captions_val = []\n",
    "image_names_val = []\n",
    "for row in testData.iterrows():\n",
    "    captions_val.append(row[1][1])\n",
    "    image_names_val.append(images_path + str(row[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> Black dog chasing brown dog through snow <end>',\n",
       " '<start> Two dogs chase each other across the snowy ground . <end>',\n",
       " '<start> Two dogs play together in the snow . <end>',\n",
       " '<start> Two dogs running through a low lying body of water . <end>',\n",
       " '<start> A little baby plays croquet . <end>']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "captions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/2513260012_03d33305cf.jpg',\n",
       " '/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/2513260012_03d33305cf.jpg',\n",
       " '/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/2513260012_03d33305cf.jpg',\n",
       " '/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/2513260012_03d33305cf.jpg',\n",
       " '/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/2903617548_d3e38d7f88.jpg']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_names[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_path):\n",
    "    img = image.load_img(image_path, target_size=(299, 299))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x /= 127.5\n",
    "    x -= 1\n",
    "    return x,image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, (299, 299))\n",
    "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
    "    return img, image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = tf.keras.applications.InceptionV3(weights='imagenet')\n",
    "new_input = inception_model.input\n",
    "encoded = inception_model.layers[-2].output\n",
    "inception_model_feat = tf.keras.Model(new_input, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inception(image_path):\n",
    "    img = preprocess_images(image_path)\n",
    "    encoding = inception_model_feat.predict(img)\n",
    "    encoding = np.reshape(encoding,encoding.shape[1])\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_train = sorted(set(image_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/1000268201_693b08cb0e.jpg',\n",
       " '/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/1001773457_577c3a7d70.jpg',\n",
       " '/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/1002674143_1b742ab4b8.jpg',\n",
       " '/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/1003163366_44323f5815.jpg',\n",
       " '/home/nishimehta15/NIC/flickr/Flickr8k_Dataset/1007129816_e794419615.jpg']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32dea3294de84423a75b2712952abf0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
    "image_dataset = image_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)\n",
    "\n",
    "for img, path in tqdm(image_dataset):\n",
    "    batch_features = inception_model_feat(img)\n",
    "    batch_features = tf.reshape(batch_features,(16,batch_features.shape[1]))\n",
    "\n",
    "    for bf, p in zip(batch_features, path):\n",
    "        path_of_feature = p.numpy().decode(\"utf-8\")\n",
    "        np.save(path_of_feature, bf.numpy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = 20000\n",
    "# caption_list = caption_list[:samples]\n",
    "# image_paths = image_paths[:samples]\n",
    "\n",
    "# encoding_train = {}\n",
    "# for image_path in tqdm(image_paths): \n",
    "#     encoding_train[image_path] = encode(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Tokenizer(num_words=5000,filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ',oov_token=\"<empty>\")\n",
    "vocab.fit_on_texts(captions)\n",
    "\n",
    "vocab.word_index['<pad>'] = 0\n",
    "vocab.index_word[0] = '<pad>'\n",
    "vocab_seqs = vocab.texts_to_sequences(captions)\n",
    "\n",
    "padded_captions = pad_sequences(vocab_seqs, padding='post')\n",
    "max_length_caption = max(len(s) for s in vocab_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.layer_output = tf.keras.layers.Dense(1)\n",
    "        self.layer1 = tf.keras.layers.Dense(units)\n",
    "        self.layer2 = tf.keras.layers.Dense(units)\n",
    "\n",
    "    def call(self, input_, hidden):\n",
    "        input_ = tf.cast(input_, tf.float32)\n",
    "        hidden = tf.cast(hidden, tf.float32)\n",
    "        hidden_layer_shape = tf.expand_dims(hidden, 1)\n",
    "        combined_input = self.layer1(input_) + self.layer2(hidden_layer_shape)\n",
    "        correlation = tf.nn.tanh(combined_input)\n",
    "#         correlation = tf.nn.tanh(combined_input)\n",
    "        attention_vector = tf.nn.softmax(self.layer_output(correlation), axis=1)\n",
    "        output_ = attention_vector * input_\n",
    "        context_vector = tf.reduce_sum(output_, axis=1)\n",
    "        return context_vector, output_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer1 = tf.keras.layers.Dense(embedding_dim)\n",
    "\n",
    "    def call(self, input_):\n",
    "        out = self.layer1(input_)\n",
    "        out = tf.nn.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.attention_layer = BahdanauAttention(units)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        self.layer1 = tf.keras.layers.Dense(units)\n",
    "        self.layer2 = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, x, features, hidden):\n",
    "        context_vector, attention_weights = self.attention_layer(features, hidden)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state = self.gru(x)\n",
    "        x = self.layer1(output)\n",
    "        x = tf.reshape(x, (-1, x.shape[2]))\n",
    "        x = self.layer2(x)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 256\n",
    "units = 512\n",
    "vocab_size = len(vocab.word_index) + 1\n",
    "\n",
    "encoder = Encoder(dim)\n",
    "decoder = Decoder(dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hidden_layer(batch_size):\n",
    "    return np.zeros((batch_size,units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_func(img_name, cap):\n",
    "    img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
    "    return img_tensor, cap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = vocab.texts_to_sequences(captions)\n",
    "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
    "max_length = max(len(t) for t in train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_names = image_names[:29984]\n",
    "cap_vector = cap_vector[:29984]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 16\n",
    "image_names = image_names[:]\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_names, cap_vector))\n",
    "\n",
    "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
    "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
    "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "# dataset = dataset.apply(tf.contrib.data.unbatch())\n",
    "# dataset = dataset.batch(BATCH_SIZE)\n",
    "# Shuffle and batch\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train_flickr\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = get_hidden_layer(target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([vocab.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = encoder(img_tensor)\n",
    "\n",
    "        for i in range(1, target.shape[1]):\n",
    "            predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
    "            loss += loss_function(target[:, i], predictions)\n",
    "            dec_input = tf.expand_dims(target[:, i], 1)\n",
    "\n",
    "    total_loss = (loss / int(target.shape[1]))\n",
    "    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
    "\n",
    "    return loss, total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 1.0798\n",
      "Epoch 1 Batch 100 Loss 0.9309\n",
      "Epoch 1 Batch 200 Loss 0.9478\n",
      "Epoch 1 Batch 300 Loss 1.0438\n",
      "Epoch 1 Batch 400 Loss 1.0253\n",
      "Epoch 1 Batch 500 Loss 1.0221\n",
      "Epoch 1 Batch 600 Loss 1.0376\n",
      "Epoch 1 Batch 700 Loss 1.0087\n",
      "Epoch 1 Batch 800 Loss 1.0766\n",
      "Epoch 1 Batch 900 Loss 1.1642\n",
      "Epoch 1 Batch 1000 Loss 0.9601\n",
      "Epoch 1 Batch 1100 Loss 0.9568\n",
      "Epoch 1 Batch 1200 Loss 0.8523\n",
      "Epoch 1 Batch 1300 Loss 0.9176\n",
      "Epoch 1 Batch 1400 Loss 0.9404\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(start_epoch, EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
    "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
    "    loss_plot.append(total_loss / num_steps)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        ckpt_manager.save()\n",
    "\n",
    "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
    "                                         total_loss/num_steps))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, 2048))\n",
    "\n",
    "    hidden = get_hidden_layer(1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
    "    img_tensor_val = inception_model_feat(temp_input)\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, [1,img_tensor_val.shape[1]])\n",
    "\n",
    "    features = encoder(img_tensor_val)\n",
    "\n",
    "    dec_input = tf.expand_dims([vocab.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result.append(vocab.index_word[predicted_id])\n",
    "\n",
    "        if vocab.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(image, result, attention_plot):\n",
    "    temp_image = np.array(Image.open(image))\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "    len_result = len(result)\n",
    "    for l in range(len_result):\n",
    "        temp_att = np.resize(attention_plot[l], (8, 8))\n",
    "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
    "        ax.set_title(result[l])\n",
    "        img = ax.imshow(temp_image)\n",
    "        ax.imshow(temp_att, cmap='gray', alpha=0.6, extent=img.get_extent())\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seqs = vocab.texts_to_sequences(captions_val)\n",
    "captions_test = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captions on the validation set\n",
    "\n",
    "rid = np.random.randint(0, len(image_names_val))\n",
    "image = image_names_val[rid]\n",
    "real_caption = ' '.join([vocab.index_word[i] for i in captions_test[rid] if i not in [0]])\n",
    "result, attention_plot = evaluate(image)\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', ' '.join(result))\n",
    "plot_attention(image, result, attention_plot)\n",
    "# opening the image\n",
    "Image.open(img_name_val[rid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
